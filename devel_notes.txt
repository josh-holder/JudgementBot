9/23/23
Back in the swing of things again. Refactored code for a3c and got some runs going.

Notably:
 - added speedup where it stores the results of evaluations, rather than recalculating to determine A3C loss
 - made model evaluation truly random
 - made naming more pythonic + added comments + added README
 - split model comparison out into separate function and logic

Did a run with 4 cores on my personal laptop. Ran for about 10 hours (25*4*22=2200 games), seemed to regress:
Generated 1004 games in 16561.51426100731 seconds.
Average Final Scores over 1004 games: [308.48605577689244, 303.08964143426294, 315.68824701195217, 316.3964143426295]

However, the results from even a quick run on the lab computer with 8 cores seemed to be promising - running right now.

Perhaps the lesson is that you need more than 4 cores worth of A3C agents to ensure data diversity and actual learning.

9/24/23
Worked deep into the morning (4am ish). Optimized agent comparisons, learned a lot about statistics and t-testing.

Implemented a much more robust and statistically valid way of comparing the models. Excited to train on the lab computer later today.

Update: ran experiments with 12 and 8 workers today. Seems that 12/20 is effectively identical to 8/25 from a training steps/min perspective,
but perhaps better from a performance perspective? Benefits maybe from seeing more diverse areas of the search space?

Not seeing clear improvement in the models, though. Giving this batch a longer time to run to see if it can
make any improvement within 10 global updates.

9/25/23
Why won't the model improve?
 - epsilon too high
 - bug in gradient propagation
 - Acceptance criteria is too harsh
 - need multi agent RL approach to make this work