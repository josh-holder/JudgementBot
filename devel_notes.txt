9/23/23
Back in the swing of things again. Refactored code for a3c and got some runs going.

Notably:
 - added speedup where it stores the results of evaluations, rather than recalculating to determine A3C loss
 - made model evaluation truly random
 - made naming more pythonic + added comments + added README
 - split model comparison out into separate function and logic

Did a run with 4 cores on my personal laptop. Ran for about 10 hours (25*4*22=2200 games), seemed to regress:
Generated 1004 games in 16561.51426100731 seconds.
Average Final Scores over 1004 games: [308.48605577689244, 303.08964143426294, 315.68824701195217, 316.3964143426295]

However, the results from even a quick run on the lab computer with 8 cores seemed to be promising - running right now.

Perhaps the lesson is that you need more than 4 cores worth of A3C agents to ensure data diversity and actual learning.

9/24/23
Worked deep into the morning (4am ish). Optimized agent comparisons, learned a lot about statistics and t-testing.

Implemented a much more robust and statistically valid way of comparing the models. Excited to train on the lab computer later today.

Update: ran experiments with 12 and 8 workers today. Seems that 12/20 is effectively identical to 8/25 from a training steps/min perspective,
but perhaps better from a performance perspective? Benefits maybe from seeing more diverse areas of the search space?

Not seeing clear improvement in the models, though. Giving this batch a longer time to run to see if it can
make any improvement within 10 global updates.

9/25/23
Why won't the model improve?
 - epsilon too high
 - bug in gradient propagation (CHECK)
 - Acceptance criteria is too harsh
 - need multi agent RL approach to make this work

So turns out that my "optimization" of returning the evaluations of the model directly was actually causing literally zero updates to be applied.
In retrospect, this is incredibly incredibly obvious. If you're not performing any computation with the model, it has no idea what inputs/outputs
led to changes in the loss function. From the model's perspective, it says "Oh, this looked like it worked well! Shame no model weights contributed
to this and we can't make any changes! :(".

Will run new training process on lab computer where weights are actually being applied, perhaps with the slightly less stringent acceptance criteria.

9/26/23
New runs still haven't seemed to improve at all. The interesting thing is that the culprit can't be the lack of multi-agent RL methods - 
without actually seeing any improvement, the best, baseline, and current models are effectively all the same (minus the gradient updates).

I supposed that could be causing the issue - it trains in an environment with 4 current agents, and then is evaluated in an environment with
2 current agents and 2 old agents. Fundamentally though, I want the new models to be able to beat the old models. I suppose I could switch to
identical setups for training and test (and thus take a 50% performance hit.) If things stall for too much longer, I might try that.

(Also, I should really read the multi-agent poker paper.)

On another note, I had the realization that if the baseline and best model are indeed the same, they should be resulting in identical score distributions.
Given that they're not, that means the evaluation process is not long enough to remove variance. I think Judgement inherently has an insane
amount of variance. Therefore, I multiplied the evaluation games by ~1- and made them much more rare. Results seem to be more consistent now,
but still not identical.